# 7. 딥러닝을 시작합니다.

## 📌 7.1 인공 신경망

>### fashion mnist data
```python
import matplotlib.pyplot as plt
fig, axs = plt.subplots(1, 10, figsize=(10,10))
for i in range (10):
  axs[i].imshow(train_input[i], cmap = 'gray_r')
  axs[i].axis("off")
plt.show()
```
![](../25_Summer_Study/images/w4/s4.md)
* 28 * 28 size의 사진이라 작고 흐릿함. 
* 0~9까지의 label은 옷 종류를 의미

>### 로지스틱 회귀로 패션 아이템 분류하기
- 확률적 경사하강법 클래스의 loss 매개변수를 'log'로 지정하여 로지스틱 손실함수를 최소화하는 SGD model 생성 가능  
    * 확률적 경사하강법은 여러 특성중 기울기가 가장 가파른 방향으로 이동하므로 정규화를 해야한다.

```python
train_scaled = train_input/255.0
train_Scaled = train_scaled.reshape(-1, 28*28) 
print(train_scaled.shape) # (60000, 784)
```
* reshape() 메서드의 두번째 매개변수를 이미지 크기에 맞게 지정하면 첫번쨰 차원 = 샘플기수는 변하지 않고 원본데이터의 두, 세번째 차원이 1차원으로 합쳐짐 

```python
from sklearn.model_selection import cross_validate
from sklearn.linear_model import SGDClassifier
sc = SGDClassifier(loss = 'log', max_iter=5, random_state=42)
scores = cross_validate(sc, train_scaled,train_target,n_jobs=-1 )
print(np.mean(scores['test_score']))
```
* 81% 성능이 나옴
> 📍 784개의 피쳐가 있으므로 로지스틱 회귀를 만들때 아주 긴 식, 아주 많은 가중치...그리고 옷의 종류에 따라 이 가중치를 다 각각 계산해야함

>### 인공신경망
![](../25_Summer_Study/images/w4/스크린샷%202025-07-27%20오후%202.23.41.png)

* 구성요소
    1) 입력층
    2) 가중치
    3) hidden layer
    4) 출력층 = 뉴런
> 인공신경망은 기존 머신러닝 알고리즘이 잘 해결하지 못한 문제에서 높은 성능을 발휘하는 새로운 머신러닝 알고리즘음!

* **tensroflow** 에서도 **keras** API를 사용할 예정
    * 인공신경망에서는 교차검증을 사용하지 않고 검증세트를 별도로 사용 cuz data가 이미 충분히 크기 때문에

```
📌 keras 설명
- 다양한 층이 있고 기분은 밀집층 = dense layer
- 양쪽의 뉴런이 모두 연결되고 있으면 FCN(fully connectd layer)
```

```python
dense = keras.layers.Dense(10, activation = 'softmax', input_shape = (784,))
```
* 첫번째 매개변수 = 뉴런의 개수 = 출력층 개수
* 출력되는 값을 확률로 바꾸기 위해 소프트맥스 함수 사용
    *  이진분류라면 sigmoid겠지만 소프트맥스도 가능
* 세번째 매겨변수는 입력층의 크기

```python
model = keras.Sequential([dense])
```
* Sequential 클래스의 객체를 만들때 앞서 만든 밀집층의 객체 dense 전달함


---
# Sequential vs Dense by Gpt

## ✅ 1. `Dense` 객체란?

### 🔹 역할: **신경망의 ‘층’(Layer)을 만드는 클래스**

* `Dense`는 Keras에서 **완전 연결층(Fully Connected Layer, FCN)** 또는 **밀집층**을 만들기 위한 클래스입니다.
* 이 층은 **모든 입력 뉴런이 모든 출력 뉴런에 연결**되어 있습니다.

### 🔹 주요 매개변수

```python
keras.layers.Dense(units, activation, input_shape)
```

| 매개변수          | 의미                                             |
| ------------- | ---------------------------------------------- |
| `units`       | 출력 뉴런(노드)의 개수 → 즉, 이 층이 출력하는 벡터의 크기            |
| `activation`  | 활성화 함수 (예: `'relu'`, `'sigmoid'`, `'softmax'`) |
| `input_shape` | 입력 벡터의 크기 (맨 앞층에서만 지정함)                        |

### 🔹 예시 설명

```python
dense = keras.layers.Dense(10, activation='softmax', input_shape=(784,))
```

* 입력: 784차원 벡터 (예: 28×28 픽셀 이미지를 1차원으로 펼침)
* 출력: 10개의 확률값 (예: 숫자 분류 문제에서 0\~9까지의 확률)
* `softmax`: 출력값의 총합이 1이 되도록 하여 다중 클래스 확률로 변환

---

## ✅ 2. `Sequential` 객체란?

### 🔹 역할: **층을 차례로 쌓은 인공신경망 모델을 만드는 클래스**

* `Sequential`은 keras의 가장 기본적인 모델 클래스입니다.
* 여러 층을 **순차적으로 연결**할 때 사용합니다.
* 내부적으로는 리스트 형태로 층을 저장함.

### 🔹 예시

```python
model = keras.Sequential([dense])
```

혹은 이렇게도 가능:

```python
model = keras.Sequential([
    keras.layers.Dense(64, activation='relu', input_shape=(784,)),
    keras.layers.Dense(10, activation='softmax')
])
```

### 🔹 특징

* 위에서 아래로 층이 순차적으로 구성됨
* 각 층의 출력이 다음 층의 입력으로 바로 연결됨
* 복잡한 구조(예: 잔차 연결, 병렬 연결 등)는 `Functional API`나 `Model` 클래스를 사용

---

## ✅ 차이점 요약: `Dense` vs `Sequential`

| 항목    | `Dense`                   | `Sequential`                 |
| ----- | ------------------------- | ---------------------------- |
| 의미    | 하나의 **레이어(층)**            | 여러 층을 쌓아서 만든 **모델 구조**       |
| 역할    | 뉴런들의 집합 (FCN, 완전 연결층) 구성  | 층들을 순서대로 연결해 하나의 전체 신경망 구성   |
| 사용 위치 | 모델 내부에서 층으로 사용            | 모델 자체로 사용되며, 여러 층을 포함        |
| 예시    | `keras.layers.Dense(...)` | `keras.Sequential([...])`    |
| 확장성   | 단일 층만 구성함                 | 여러 층을 포함할 수 있으며, 학습/평가/예측 가능 |

---

>### 인공신경망으로 mnist 분류하기


#### ✅ `model.compile()`이란?

🔹 역할: 모델의 학습 방법을 지정

* 손실 함수 (`loss`)
* 최적화 알고리즘 (`optimizer`)
* 평가 지표 (`metrics`)

---

✅ 기본 구조

```python
model.compile(
    optimizer='adam',
    loss='sparse_categorical_crossentropy',
    metrics=['accuracy']
)
```

---

✅ 각 매개변수 설명

| 매개변수        | 의미                                                                              |
| ----------- | ------------------------------------------------------------------------------- |
| `optimizer` | 모델의 가중치를 **어떻게 갱신**할지 결정하는 알고리즘<br>(ex. `'sgd'`, `'adam'`, `'rmsprop'`, ...)    |
| `loss`      | 모델이 얼마나 잘못 예측했는지 계산하는 **손실 함수**<br>(ex. `categorical_crossentropy`, `mse`, ...) |
| `metrics`   | 학습 및 평가할 때 확인할 **성능 지표**<br>(보통 `'accuracy'`, `'mae'` 등 사용)                     |

---

✅ 예시

📌 분류 문제 (다중 클래스)

```python
model.compile(
    optimizer='adam',
    loss='sparse_categorical_crossentropy',
    metrics=['accuracy']
)
```

📌 회귀 문제

```python
model.compile(
    optimizer='adam',
    loss='mean_squared_error',   # 또는 'mse'
    metrics=['mae']              # 평균 절대 오차
)
```

📌 이진 분류 문제

```python
model.compile(
    optimizer='adam',
    loss='binary_crossentropy',
    metrics=['accuracy']
)
```

---

✅ 필요성

`compile()`을 하지 않으면 `fit()`이나 `evaluate()`를 쓸 수 없어요.
즉, **학습이 불가능**

---

#### ✅ 요약

| 단계           | 함수                          | 역할                     |
| ------------ | --------------------------- | ---------------------- |
| 1️⃣ 모델 구조 정의 | `Sequential()` 또는 `Model()` | 신경망 구조 만들기             |
| 2️⃣ 학습 설정    | `compile()`                 | 손실 함수, 옵티마이저, 평가 지표 지정 |
| 3️⃣ 학습 시작    | `fit()`                     | 실제 데이터를 넣어 학습 시작       |

---

```python
model.compile(loss = 'sparse_categorical_crossentropy', metrics = ['accuracy'])
model.fit(train_scaled, train_target, epochs=5)
```

* keras에서 성능 평가하는 메서드 = evaluate()

# 손실함수 정리
좋습니다! 지금까지 다뤘던 손실 함수 관련 내용을 모두 정리하고, 추가로 \*\*`from_logits=True`\*\*까지 포함해서 **딥러닝 손실 함수 완전 정리판**을 만들어드릴게요.
처음 공부하는 사람도 이해할 수 있도록 구성할게요.

---

# 📘 손실 함수 (Loss Function) 완전 정리

---

## ✅ 1. 손실 함수란?

> **손실 함수 = 예측값과 정답 사이의 차이를 수치로 나타낸 함수**

* 학습할 때 모델이 얼마나 틀렸는지를 수치화함
* 이 손실값을 줄이도록 \*\*모델의 가중치를 조정(역전파)\*\*함

---

## ✅ 2. 분류 문제에서 많이 쓰는 손실 함수 3종

| 이름                                | 용도             | 정답 형식         | 출력층 활성화 |
| --------------------------------- | -------------- | ------------- | ------- |
| `binary_crossentropy`             | 이진 분류 (0 또는 1) | 정수(0/1) or 확률 | sigmoid |
| `categorical_crossentropy`        | 다중 클래스 분류      | **원-핫 벡터**    | softmax |
| `sparse_categorical_crossentropy` | 다중 클래스 분류      | **정수 라벨**     | softmax |

---

## ✅ 3. `categorical_crossentropy` vs `sparse_categorical_crossentropy`

| 항목         | `categorical_crossentropy`  | `sparse_categorical_crossentropy` |
| ---------- | --------------------------- | --------------------------------- |
| 정답 라벨 형식   | 원-핫 인코딩 `[0, 0, 1, 0, ...]` | 정수 인덱스 `2`                        |
| 원-핫 인코딩 필요 | ✅ 필요                        | ❌ 불필요                             |
| 내부 처리 방식   | 직접 계산                       | 내부적으로 자동 인코딩 후 계산                 |
| 사용 편의성     | 불편 (변환 필요)                  | 편함 (정수 그대로 사용 가능)                 |

---

## ✅ 4. softmax와 crossentropy 관계

### 🔹 `softmax`

* 출력층에서 사용되는 함수
* 입력값(logits)을 확률로 변환
* 모든 클래스 확률 합 = 1

### 🔹 `crossentropy`

* 예측 확률과 정답 간의 차이 계산
* 정답 클래스의 확률이 높을수록 손실 작아짐

---

## ✅ 5. `from_logits=True`란?

### 🔸 logits란?

> **logits = softmax 전에 계산된 "가공되지 않은 출력값"**

예: 모델이 마지막에 `Dense(10)`만 있고 softmax가 없을 때
→ 출력값은 \[2.3, -1.2, 0.7, ...] 같은 **원시 점수**

### 🔸 문제가 발생하는 경우

* `softmax`가 없는 상태에서 `categorical_crossentropy`를 그냥 쓰면
  ➤ **숫자가 확률이 아니기 때문에 손실 계산이 잘못됨**

### 🔸 해결 방법

> **손실 함수에 `from_logits=True`를 설정**하면, 내부에서 `softmax`를 자동으로 적용한 후 crossentropy 계산

---

### ✅ 예시 코드 비교

#### ① softmax 포함 → `from_logits=False` (기본)

```python
model = keras.Sequential([
    keras.layers.Dense(10, activation='softmax')
])

model.compile(
    loss='sparse_categorical_crossentropy',  # from_logits=False가 기본
    optimizer='adam'
)
```

#### ② softmax 없음 → `from_logits=True`

```python
model = keras.Sequential([
    keras.layers.Dense(10)  # activation 없음 = logits
])

loss_fn = keras.losses.SparseCategoricalCrossentropy(from_logits=True)

model.compile(
    loss=loss_fn,
    optimizer='adam'
)
```

---

## ✅ 6. crossentropy 수식 요약

### One-hot 기준 (categorical)

```
loss = -∑(y_true × log(y_pred))
```

### 정수 기준 (sparse)

```
loss = -log(y_pred[정답 인덱스])
```

### 예시:

```python
예측 확률: [0.1, 0.2, 0.7]
정답 인덱스: 2

손실 = -log(0.7)
```

---

## ✅ 7. 이진 분류에서 `binary_crossentropy`

* 출력층: `Dense(1, activation='sigmoid')`
* 정답: `0` 또는 `1`
* 손실:

```
loss = -[y*log(p) + (1-y)*log(1-p)]
```

* 만약 sigmoid 생략 시 → `from_logits=True` 사용해야 함

---

## ✅ 8. 시각적 비교 요약

| 모델 구조                           | 손실 함수                                             | 정답 라벨                | `from_logits` 설정 |
| ------------------------------- | ------------------------------------------------- | -------------------- | ---------------- |
| Dense(10, activation='softmax') | `sparse_categorical_crossentropy`                 | \[3, 2, 1, 0, ...]   | ❌ (기본값)          |
| Dense(10) (소프트맥스 없음)            | `SparseCategoricalCrossentropy(from_logits=True)` | \[3, 2, 1, ...]      | ✅ 꼭 필요           |
| Dense(10, activation='softmax') | `categorical_crossentropy`                        | \[\[0,0,1,...], ...] | ❌ (기본값)          |
| Dense(1, activation='sigmoid')  | `binary_crossentropy`                             | \[0, 1, 0, 1]        | ❌                |
| Dense(1)                        | `BinaryCrossentropy(from_logits=True)`            | \[0, 1, 0, 1]        | ✅                |

---

## 📌 7.2 심층 신경망

**활성화 함수의 역할**

- 은닉층 유닛에 비선형성 도입
- 예: ReLU → 음수는 0, 양수는 그대로 통과 → 학습 가능성 증가
```python
dense1 = keras.layers.Dense(100, activation='sigmoid', input_shape=(784,))
dense2 = keras. layers.Dense(10, activation='softmax')
```

**은닉층 (dense1)**

- `keras.layers.Dense(100, activation='sigmoid', input_shape=(784,))`
- 뉴런 수: 100개
- 활성화 함수: `'sigmoid'`
- 입력 크기 지정: 첫 번째 층이므로 `input_shape=(784,)` 명시 필요
- 주의 사항: 은닉층의 뉴런 수는 출력층 뉴런보다 많아야 정보 손실을 줄일 수 있음
- 뉴런 수 결정: 명확한 기준은 없으며 경험과 실험에 의존

**출력층 (dense2)**

- `keras.layers.Dense(10, activation='softmax')`
- 뉴런 수: 10개 (10개 클래스 예측)
- 활성화 함수: `'softmax'` → 각 클래스 확률 출력

심층 신경망 만들기
```python
model = keras.Sequential([dense1, dense2])
model.summary()
```
>### 미니배치 경사 하강법

전체 데이터를 작은 묶음으로 나누어 한 번에 하나의 묶음씩 사용하여 모델을 학습시키는 방법

- 전체 훈련 데이터를 작은 배치로 나눔
    
    예: 전체 60,000개 중 배치 크기 32이면 → 1 epoch에 1,875번 반복
    
- 각 미니배치에 대해
    
    예측 → 손실 계산 → 기울기 계산 → 가중치 업데이트
    
- 이 과정을 여러 epoch 동안 반복

### RELU수

* 특징:
양수는 그대로 출력, 음수는 0으로 출력
* 효과: 계산 단순, 비선형성 유지, 기울기 소멸 문제 완해
    *  이미지 처리 분야에서 매우 효과적
```python
model = keras.Sequential()
model.add(keras.layers.Flatten(input_shape=(28, 28)))
model.add(keras.layers.Dense(100, activation='relu'))
model.add(keras.layers.Dense(10, activation='softmax'))

model.summary()
```