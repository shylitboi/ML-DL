# 트리 알고리즘

## 📌 결정 트리
: 이유를 설명하기 쉬운 모델

```python
from sklearn.tree import DecisionTreeClassifier
dt = DecisionTreeClassifier(random_state=42)
dt.fit(train_scaled, train_target)
print(dt.score(train_scaled, train_target))
print(dt.score(test_scaled, test_target))
```
-> 높은 정확도..!!

-> 결정 트리를 이해하기 위해 **plot_tree()** 함수 사용하여 트리 그림으로 출력
```
import matplotlib.pyplot as plt
from sklearn.tree import plot_tree
plt.figure(figsize=(10,8))
plot_tree(dt) 
plt.show()
```

![](../25_Summer_Study/images/w2/다운로드%20(3).png)
* 보기 복잡하므로 제한하여 출력

```
plt.figure(figsize=(10,8))
plot_tree(dt, max_depth=1, filled=True, feature_names=['alcohol','sugar','pH'])
plt.show()  
```
![](../25_Summer_Study/images/w2/다운로드%20(4).png)
* 루트 노드 : 맨 위의 노드 -> 당도값 필터링
* filled=True 옵션으로 클래스마다 색깔을 부여하고 클래스 비율이 높아지면 진한색으로 표시하는 옵션

>### 지니 불순도
지니불순도 = 1-(음성클래스비율^2 + 양성클래스비율^2)

해석
```
0: 완전히 순수함 (한 클래스만 존재) = 순수 노드

0.5: 두 클래스가 반반 섞여 있을 때 최대

1에 가까움: 다수의 클래스가 고르게 섞여 있음
```

> 📍 결정트리 모델은 부모 노드와 자식노드간 불순도 차이가 가능한 크도록(정보 이득이 크드록) 트리를 성장시킨다!!

* 다른 불순도 기준 : 엔트로피 불순도(criterion = 'entropy'), 성능 차이는 크지 않음
```
dt2 = DecisionTreeClassifier(random_state=42, criterion='entropy')
dt2.fit(train_scaled, train_target)
print(dt2.score(train_scaled, train_target))
print(dt2.score(test_scaled, test_target))
```

>### 가지치기 by max_depth 설정
: 가지 치기를 하지 않는다면 무작정 자라나는 트리가 생성 = 과적합 확률 up
-> 돌려보니 훈련 세트의 성능은 낮아졌지만 테스트 성능은 그대로

![]()
> 깊이 3에 있는 최종 리프 노드에서 왼쪽 세번째의 노드만 음성 클래스가 더 많음. 이 노드에 도착해야지 음성(레드와인)으로 예측

**📍 이때 당도가 음수로나오는 문제 발생...!! -> 표준화 했기 때문임**


### 결정 트리 특징
   
 1. 스케일은 결정 트리에서 영향을 미치지 않기 때문에 표준화 전처리를 할 필요가 없음 오히려 해석에 어려움을 줄수도 있음.


    ```
    dt = DecisionTreeClassifier(random_state=42, max_depth=3)
    dt.fit(train_input, train_target)
    print(dt.score(train_input, train_target))
    print(dt.score(test_input, test_target))  
    ```
    위와 같이 모델 설정하였을 때 아래와 같은 결과가 나옴
    ![]()

    **📍 특성값을 표준화 하지 않았기 때문에 이해하기 훨씬 쉽다**
2. feature_importances_ :   어떤 특성이 가장 유용한지 알려줌

    [0.12345626 0.86862934 0.0079144 ]

    두번째 특성인 당도가 가장 중요함(0.87)


