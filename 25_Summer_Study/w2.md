# 5. 트리 알고리즘

## 📌 5.1 결정 트리
: 이유를 설명하기 쉬운 모델

```python
from sklearn.tree import DecisionTreeClassifier
dt = DecisionTreeClassifier(random_state=42)
dt.fit(train_scaled, train_target)
print(dt.score(train_scaled, train_target))
print(dt.score(test_scaled, test_target))
```
-> 높은 정확도..!!

-> 결정 트리를 이해하기 위해 **plot_tree()** 함수 사용하여 트리 그림으로 출력
```
import matplotlib.pyplot as plt
from sklearn.tree import plot_tree
plt.figure(figsize=(10,8))
plot_tree(dt) 
plt.show()
```

![](../25_Summer_Study/images/w2/다운로드%20(3).png)
* 보기 복잡하므로 제한하여 출력

```
plt.figure(figsize=(10,8))
plot_tree(dt, max_depth=1, filled=True, feature_names=['alcohol','sugar','pH'])
plt.show()  
```
![](../25_Summer_Study/images/w2/다운로드%20(4).png)
* 루트 노드 : 맨 위의 노드 -> 당도값 필터링
* filled=True 옵션으로 클래스마다 색깔을 부여하고 클래스 비율이 높아지면 진한색으로 표시하는 옵션

>### 지니 불순도
지니불순도 = 1-(음성클래스비율^2 + 양성클래스비율^2)

해석
```
0: 완전히 순수함 (한 클래스만 존재) = 순수 노드

0.5: 두 클래스가 반반 섞여 있을 때 최대

1에 가까움: 다수의 클래스가 고르게 섞여 있음
```

> 📍 결정트리 모델은 부모 노드와 자식노드간 불순도 차이가 가능한 크도록(정보 이득이 크드록) 트리를 성장시킨다!!

* 다른 불순도 기준 : 엔트로피 불순도(criterion = 'entropy'), 성능 차이는 크지 않음
```
dt2 = DecisionTreeClassifier(random_state=42, criterion='entropy')
dt2.fit(train_scaled, train_target)
print(dt2.score(train_scaled, train_target))
print(dt2.score(test_scaled, test_target))
```

>### 가지치기 by max_depth 설정
: 가지 치기를 하지 않는다면 무작정 자라나는 트리가 생성 = 과적합 확률 up
-> 돌려보니 훈련 세트의 성능은 낮아졌지만 테스트 성능은 그대로

![](../25_Summer_Study/images/w2/다운로드%20(5).png)
> 깊이 3에 있는 최종 리프 노드에서 왼쪽 세번째의 노드만 음성 클래스가 더 많음. 이 노드에 도착해야지 음성(레드와인)으로 예측

**📍 이때 당도가 음수로나오는 문제 발생...!! -> 표준화 했기 때문임**


### 결정 트리 특징
   
 1. 스케일은 결정 트리에서 영향을 미치지 않기 때문에 표준화 전처리를 할 필요가 없음 오히려 해석에 어려움을 줄수도 있음.


    ```
    dt = DecisionTreeClassifier(random_state=42, max_depth=3)
    dt.fit(train_input, train_target)
    print(dt.score(train_input, train_target))
    print(dt.score(test_input, test_target))  
    ```
    위와 같이 모델 설정하였을 때 아래와 같은 결과가 나옴
    ![](../25_Summer_Study/images/w2/다운로드%20(6).png)

    **📍 특성값을 표준화 하지 않았기 때문에 이해하기 훨씬 쉽다**
2. feature_importances_ :   어떤 특성이 가장 유용한지 알려줌

    [0.12345626 0.86862934 0.0079144 ]

    두번째 특성인 당도가 가장 중요함(0.87)

## 📌 5.2 교차 검증과 그리드 서치

>###  검증 세트
: 테스트 세트를 사용하지 않고 과대/과소 적합인지 판단하는 방법, 보통 6 : 2 : 2 비율로 사용

**먼저 8:2로 테스트 세트 분리 뒤 훈련 세트에서 20%를 다시 뗴어 검증 세트로 만든다**

```
from sklearn.model_selection import train_test_split
train_input, test_input, train_target, test_target = train_test_split(data, target, test_size=0.2, random_state=42)
```
* 한번 더 나눈다
```
sub_input, val_input, sub_target, val_target = train_test_split(train_input, train_target, test_size=0.2, random_state=42)
print(sub_input.shape, val_input.shape, test_input.shape)
```
>### k-fold 교차 검증 : 안정적인 검증 세트 점수를 위해 사용
* default : k = 5, cv 매개변수로 폴드 수 변경 가능
```
from sklearn.model_selection import cross_validate
scores = cross_validate(dt, train_input, train_target)
print(scores)
```

**📌 훈련 세트 섞을라면 분할기(splitter) 지정**
>분류 모델의 경우 타깃 클래스가 골고루 섞이기 위해선 StratifiedKFold 사용

```
from sklearn.model_selection import StratifiedKFold
splitter = StratifiedKFold(n_splits=10, shuffle=True, random_state=42)
scores = cross_validate(dt, train_input, train_target, cv=splitter)
print(np.mean(scores['test_score']))  
```
**splitter = StratifiedKFold() 작성 후 cv = splitter로 넣기**

>### 하이퍼 파라미터 튜닝 by GridSearch CV

* GridSearchCV : 하이퍼파라미터 튜닝 + 교차 검증

**📍 min_impurity_decrease 매개변수의 최적값 찾아보기**
```
from sklearn.model_selection import GridSearchCV
params = {'min_impurity_decrease' : [0.0001, 0.0002, 0.0003, 0.0004, 0.0005]}
gs = GridSearchCV(DecisionTreeClassifier(random_state=42), params, n_jobs=-1, cv=5)
gs.fit(train_input, train_target)
```
    * n_jobs : 병렬 실행에 사용할 일꾼(코어)수, -1로 설정시 모든 코어 사용

| 항목    | `gs.best_estimator_` | `gs.best_params_` |
| ----- | -------------------- | ----------------- |
| 의미    | 최적 모델 객체             | 최적 하이퍼파라미터 조합     |
| 타입    | 모델 클래스 인스턴스          | 딕셔너리 (`dict`)     |
| 사용 목적 | 예측 및 평가에 사용          | 최적 파라미터 확인 및 로깅   |
| 예시    | `model.predict()`    | `print(params)`   |


**📍 좀 더 복잡한 예제**

```
params = {'min_impurity_decrease' : np.arange(0.0001, 0.001, 0.0001), # 0.0001부터 시작해서 0,001까지 0.0001 늘리면서 탐색
          'max_depth' : range(5,20,1),
          'min_samples_split': range(2,100,10)}
gs = GridSearchCV(DecisionTreeClassifier(random_state=42), params, n_jobs=-1, cv=5)
gs.fit(train_input, train_target) 
```
>### 랜덤서치 : 매개변수값 목록(리스트) 가 아닌 **확률분포 객체**를 전달, 시간 줄어듬

* scipy.stats 라이브러리에서 확률 분포 임포트 후 사용한다
```
params = {'min_impurity_decrease' : uniform(0.0001, 0.001), 
          'max_depth' :randint(20,50),
          'min_samples_split': randint(2,25),
          'min_samples_leaf': randint(1,25)}
```
* 샘플링횟수는 n_iter 매개변수에서 지정함

```
from sklearn.model_selection import RandomizedSearchCV
gs = RandomizedSearchCV(DecisionTreeClassifier(random_state=42), params, n_iter=100, n_jobs=-1, cv=5,random_state=42)
gs.fit(train_input, train_target)
```
```
dt = gs.best_estimator_
print(dt.score(test_input, test_target))
```

## 📌 앙상블

>### 랜덤포레스트

* 정형 데이터 : 데이터베이스, 엑셀로 표현할 수 있는 데이터 -> 앙상블 학습이 가장 뛰어난 성과를 보낸다

* 앙상블 모델은 대부분 결정트리를 기반으로 만들어져 있음

* 개념
    1) 결정 트리를 랜덤하게 만들어 숲을 만듬
    2) **부트스트랩 샘플** 훈련 데이터셋을 복원 추출하는 샘플, 1000개의 가방에서 100개씩 샘플을 복원추출해서 총 1000개 뽑음
    3) 각 노드를 분할 할 때 전체 피쳐 개수의 제곱근 만큼(4개 피쳐라면 2개) 선택해서 분할 특성 사용
        * 다만 RandomForestRegressor는 전체 특성 사용
    4) 각 트리의 클래스별 확률을 평균하여 가장 높은 확률을 가진 클래스를 예측치로 사용
    5) 과적합 방지, 안정적인 성능

* 실습
    * n_jobs 는 -1 을 사용하는것이 좋다

```
from sklearn.ensemble import RandomForestClassifier
rf = RandomForestClassifier(n_jobs=-1, random_state=42)
scores = cross_validate(rf, train_input, train_target, return_train_score=True, n_jobs=-1)
print(np.mean(scores['train_score']), np.mean(scores['test_score']))
```

**📍OOB 샘플 : 부트스트랩 샘플에 포함되지 않고 남은 애들 -> 검증세트 역할 가능**
    -> oob_score =True 설정

```
rf = RandomForestClassifier(oob_score=True, n_jobs=-1, random_state=42)
rf.fit(train_input, train_target)
print(rf.oob_score_)
```
>### 그래디언트 부스팅

* 개념
    1) 깊이가 얕은 결정트리를 사용하여 이전 트리 오차 보완
    2) 경사 하강법을 사용하여 트리를 앙상블에 추가
    3) 과적합에 매우 강함

**gpt 설명**

```
# 🌲 Gradient Boosting (그래디언트 부스팅)

## 📌 개요

**그래디언트 부스팅(Gradient Boosting)**은 여러 개의 **약한 학습기(주로 작은 결정 트리)**를 순차적으로 훈련시켜, 이전 모델이 만든 **오차를 줄여나가며 성능을 향상**시키는 앙상블 학습 기법입니다.

> 핵심 아이디어:  
> **"이전 모델이 틀린 부분을 다음 모델이 예측하도록 훈련하고, 그 결과를 조금씩 더해 전체 예측을 보정한다."**
```

## 🔁 작동 원리 (회귀 예시, 분류 모델에서는 예측값과 실제값 사이의 손실 함수의 미분값(= 그래디언트)을 타깃으로 새 모델을 학습)

### Step 1: 초기 예측값 설정
- 모든 샘플에 대해 같은 초기값 예측 (예: 평균값)

```text
ŷ₀ = 전체 y의 평균
````

### Step 2: 잔차(residual) 계산

* 실제값과 예측값의 차이 = 모델이 틀린 정도

```text
residual r₁ = y - ŷ₀
```

### Step 3: 잔차를 예측하는 약한 모델 학습

* 새 트리 모델을 학습시켜 **r₁을 예측**하게 함

### Step 4: 예측값 업데이트

* 예측값에 새로운 모델의 결과를 **learning rate만큼 곱해서 더함**

```text
ŷ₁ = ŷ₀ + α × h₁(x)
```

* α는 **learning rate** (작을수록 학습 속도는 느리지만 안정적)

### Step 5: 반복

* 매번 새 잔차를 계산하고, 잔차를 예측하는 새 모델을 학습 → 예측 보정
* 전체 모델:

```text
Final prediction:
ŷ = ŷ₀ + α·h₁(x) + α·h₂(x) + α·h₃(x) + ... + α·hₙ(x)
```

---

## 📊 예시 테이블 (회귀)

| X | y (실제값) | 초기 예측 ŷ₀ | 오차 r₁ | 트리1 예측 | 업데이트된 예측 ŷ₁          |
| - | ------- | -------- | ----- | ------ | -------------------- |
| A | 80      | 66.7     | +13.3 | +10    | 66.7 + 0.1×10 = 67.7 |
| B | 50      | 66.7     | -16.7 | -15    | 66.7 - 1.5 = 65.2    |
| C | 70      | 66.7     | +3.3  | +5     | 66.7 + 0.5 = 67.2    |

---

## ✅ 장점

* 🔥 **고성능**: 많은 실제 문제에서 뛰어난 예측력
* 🔍 **유연함**: 회귀, 이진 분류, 다중 분류 등 다양한 손실 함수 사용 가능
* 📉 **자동 특성 선택**: 중요하지 않은 변수는 무시되는 경향
* 🧠 **과적합 제어 가능**: 학습률, 트리 수, 트리 깊이 조절로 조정 가능

---

## ⚠️ 단점

* 🐢 **학습 속도 느림**: 트리를 순차적으로 학습하므로 병렬화 어려움
* 🛠 **하이퍼파라미터 튜닝 필요**: 성능 최적화를 위해 세밀한 조정 필요
* 🎭 **복잡성 높음**: 모델 해석이 어려움 (하지만 feature importance로 일부 해결 가능)
* 🤕 **과적합 위험**: 트리 수가 많거나 과하게 깊으면 과적합될 수 있음

---

## 🚀 대표 구현 라이브러리

| 라이브러리            | 특징                          |
| ---------------- | --------------------------- |
| **XGBoost**      | 빠르고 성능 우수, 정교한 튜닝 가능        |
| **LightGBM**     | 대용량 데이터에 최적화, 빠름            |
| **CatBoost**     | 범주형 데이터 처리에 강함              |
| **scikit-learn** | 기본적인 GradientBoosting 구현 제공 |

---

## 📌 참고 키워드

* **Residual**: 실제값과 예측값의 차이 → 다음 모델이 예측해야 할 타깃
* **Learning Rate (α)**: 각 트리의 기여도를 조절
* **Boosting**: 약한 모델을 모아 강한 모델을 만드는 방식

---

## 📝 관련 개념 비교

| 개념           | 설명                                |
| ------------ | --------------------------------- |
| **Bagging**  | 병렬적으로 여러 모델 학습 (ex: 랜덤 포레스트)      |
| **Boosting** | 순차적으로 모델 학습, 이전 모델의 오차를 다음 모델이 보정 |
| **Stacking** | 여러 모델의 예측을 메타 모델이 조합              |

---

* 실습

```
from sklearn.ensemble import GradientBoostingClassifier
gb = GradientBoostingClassifier(random_state=42, n_estimators=500, learning_rate=0.2)
scores = cross_validate(gb, train_input, train_target, return_train_score=True, n_jobs=-1)
print(np.mean(scores['train_score']), np.mean(scores['test_score']))
```
-> tree를 500개나 사용했지만 과적합이 잘 되지 않고 성능이 좋음

#### Gradient Boosting  모델들

> 📍 XGB
```
from xgboost import XGBClassifier
xgb =XGBClassifier(tree_method = 'hist', random_state=42)
scroes = cross_validate(xgb, train_input, train_target, return_train_score=True, n_jobs=-1)
print(np.mean(scores['train_score']), np.mean(scores['test_score']))  
```
> 📍 LightGBM
```
from lightgbm import LGBMClassifier
lgb = LGBMClassifier(random_state=42)
scores = cross_validate(lgb, train_input, train_target, return_train_score=True, n_jobs=-1)
print(np.mean(scores['train_score']), np.mean(scores['test_score']))
```
