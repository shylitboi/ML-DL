# 5. íŠ¸ë¦¬ ì•Œê³ ë¦¬ì¦˜

## ğŸ“Œ 5.1 ê²°ì • íŠ¸ë¦¬
: ì´ìœ ë¥¼ ì„¤ëª…í•˜ê¸° ì‰¬ìš´ ëª¨ë¸

```python
from sklearn.tree import DecisionTreeClassifier
dt = DecisionTreeClassifier(random_state=42)
dt.fit(train_scaled, train_target)
print(dt.score(train_scaled, train_target))
print(dt.score(test_scaled, test_target))
```
-> ë†’ì€ ì •í™•ë„..!!

-> ê²°ì • íŠ¸ë¦¬ë¥¼ ì´í•´í•˜ê¸° ìœ„í•´ **plot_tree()** í•¨ìˆ˜ ì‚¬ìš©í•˜ì—¬ íŠ¸ë¦¬ ê·¸ë¦¼ìœ¼ë¡œ ì¶œë ¥
```
import matplotlib.pyplot as plt
from sklearn.tree import plot_tree
plt.figure(figsize=(10,8))
plot_tree(dt) 
plt.show()
```

![](../25_Summer_Study/images/w2/á„ƒá…¡á„‹á…®á†«á„…á…©á„ƒá…³%20(3).png)
* ë³´ê¸° ë³µì¡í•˜ë¯€ë¡œ ì œí•œí•˜ì—¬ ì¶œë ¥

```
plt.figure(figsize=(10,8))
plot_tree(dt, max_depth=1, filled=True, feature_names=['alcohol','sugar','pH'])
plt.show()  
```
![](../25_Summer_Study/images/w2/á„ƒá…¡á„‹á…®á†«á„…á…©á„ƒá…³%20(4).png)
* ë£¨íŠ¸ ë…¸ë“œ : ë§¨ ìœ„ì˜ ë…¸ë“œ -> ë‹¹ë„ê°’ í•„í„°ë§
* filled=True ì˜µì…˜ìœ¼ë¡œ í´ë˜ìŠ¤ë§ˆë‹¤ ìƒ‰ê¹”ì„ ë¶€ì—¬í•˜ê³  í´ë˜ìŠ¤ ë¹„ìœ¨ì´ ë†’ì•„ì§€ë©´ ì§„í•œìƒ‰ìœ¼ë¡œ í‘œì‹œí•˜ëŠ” ì˜µì…˜

>### ì§€ë‹ˆ ë¶ˆìˆœë„
ì§€ë‹ˆë¶ˆìˆœë„ = 1-(ìŒì„±í´ë˜ìŠ¤ë¹„ìœ¨^2 + ì–‘ì„±í´ë˜ìŠ¤ë¹„ìœ¨^2)

í•´ì„
```
0: ì™„ì „íˆ ìˆœìˆ˜í•¨ (í•œ í´ë˜ìŠ¤ë§Œ ì¡´ì¬) = ìˆœìˆ˜ ë…¸ë“œ

0.5: ë‘ í´ë˜ìŠ¤ê°€ ë°˜ë°˜ ì„ì—¬ ìˆì„ ë•Œ ìµœëŒ€

1ì— ê°€ê¹Œì›€: ë‹¤ìˆ˜ì˜ í´ë˜ìŠ¤ê°€ ê³ ë¥´ê²Œ ì„ì—¬ ìˆìŒ
```

> ğŸ“ ê²°ì •íŠ¸ë¦¬ ëª¨ë¸ì€ ë¶€ëª¨ ë…¸ë“œì™€ ìì‹ë…¸ë“œê°„ ë¶ˆìˆœë„ ì°¨ì´ê°€ ê°€ëŠ¥í•œ í¬ë„ë¡(ì •ë³´ ì´ë“ì´ í¬ë“œë¡) íŠ¸ë¦¬ë¥¼ ì„±ì¥ì‹œí‚¨ë‹¤!!

* ë‹¤ë¥¸ ë¶ˆìˆœë„ ê¸°ì¤€ : ì—”íŠ¸ë¡œí”¼ ë¶ˆìˆœë„(criterion = 'entropy'), ì„±ëŠ¥ ì°¨ì´ëŠ” í¬ì§€ ì•ŠìŒ
```
dt2 = DecisionTreeClassifier(random_state=42, criterion='entropy')
dt2.fit(train_scaled, train_target)
print(dt2.score(train_scaled, train_target))
print(dt2.score(test_scaled, test_target))
```

>### ê°€ì§€ì¹˜ê¸° by max_depth ì„¤ì •
: ê°€ì§€ ì¹˜ê¸°ë¥¼ í•˜ì§€ ì•ŠëŠ”ë‹¤ë©´ ë¬´ì‘ì • ìë¼ë‚˜ëŠ” íŠ¸ë¦¬ê°€ ìƒì„± = ê³¼ì í•© í™•ë¥  up
-> ëŒë ¤ë³´ë‹ˆ í›ˆë ¨ ì„¸íŠ¸ì˜ ì„±ëŠ¥ì€ ë‚®ì•„ì¡Œì§€ë§Œ í…ŒìŠ¤íŠ¸ ì„±ëŠ¥ì€ ê·¸ëŒ€ë¡œ

![](../25_Summer_Study/images/w2/á„ƒá…¡á„‹á…®á†«á„…á…©á„ƒá…³%20(5).png)
> ê¹Šì´ 3ì— ìˆëŠ” ìµœì¢… ë¦¬í”„ ë…¸ë“œì—ì„œ ì™¼ìª½ ì„¸ë²ˆì§¸ì˜ ë…¸ë“œë§Œ ìŒì„± í´ë˜ìŠ¤ê°€ ë” ë§ìŒ. ì´ ë…¸ë“œì— ë„ì°©í•´ì•¼ì§€ ìŒì„±(ë ˆë“œì™€ì¸)ìœ¼ë¡œ ì˜ˆì¸¡

**ğŸ“ ì´ë•Œ ë‹¹ë„ê°€ ìŒìˆ˜ë¡œë‚˜ì˜¤ëŠ” ë¬¸ì œ ë°œìƒ...!! -> í‘œì¤€í™” í–ˆê¸° ë•Œë¬¸ì„**


### ê²°ì • íŠ¸ë¦¬ íŠ¹ì§•
   
 1. ìŠ¤ì¼€ì¼ì€ ê²°ì • íŠ¸ë¦¬ì—ì„œ ì˜í–¥ì„ ë¯¸ì¹˜ì§€ ì•Šê¸° ë•Œë¬¸ì— í‘œì¤€í™” ì „ì²˜ë¦¬ë¥¼ í•  í•„ìš”ê°€ ì—†ìŒ ì˜¤íˆë ¤ í•´ì„ì— ì–´ë ¤ì›€ì„ ì¤„ìˆ˜ë„ ìˆìŒ.


    ```
    dt = DecisionTreeClassifier(random_state=42, max_depth=3)
    dt.fit(train_input, train_target)
    print(dt.score(train_input, train_target))
    print(dt.score(test_input, test_target))  
    ```
    ìœ„ì™€ ê°™ì´ ëª¨ë¸ ì„¤ì •í•˜ì˜€ì„ ë•Œ ì•„ë˜ì™€ ê°™ì€ ê²°ê³¼ê°€ ë‚˜ì˜´
    ![](../25_Summer_Study/images/w2/á„ƒá…¡á„‹á…®á†«á„…á…©á„ƒá…³%20(6).png)

    **ğŸ“ íŠ¹ì„±ê°’ì„ í‘œì¤€í™” í•˜ì§€ ì•Šì•˜ê¸° ë•Œë¬¸ì— ì´í•´í•˜ê¸° í›¨ì”¬ ì‰½ë‹¤**
2. feature_importances_ :   ì–´ë–¤ íŠ¹ì„±ì´ ê°€ì¥ ìœ ìš©í•œì§€ ì•Œë ¤ì¤Œ

    [0.12345626 0.86862934 0.0079144 ]

    ë‘ë²ˆì§¸ íŠ¹ì„±ì¸ ë‹¹ë„ê°€ ê°€ì¥ ì¤‘ìš”í•¨(0.87)
>### í”¼ì³ ì¤‘ìš”ë„ vs Permutation importance

| êµ¬ë¶„         | íŠ¸ë¦¬ ê¸°ë°˜ í”¼ì²˜ ì¤‘ìš”ë„        | Permutation Importance |
| ---------- | ------------------- | ---------------------- |
| **ê¸°ë°˜ ë°©ì‹**  | ë…¸ë“œ ë¶„í•  ì‹œ ë¶ˆìˆœë„ ê°ì†ŒëŸ‰     | ì˜ˆì¸¡ ì„±ëŠ¥ ë³€í™”               |
| **ê³„ì‚° ì‹œì **  | í›ˆë ¨ ì¤‘                | í›ˆë ¨ í›„                   |
| **ì†ë„**     | ë¹ ë¦„                  | ëŠë¦¼                     |
| **í¸í–¥**     | ìˆìŒ (ë²”ì£¼ ìˆ˜ ë§ì€ í”¼ì²˜ì— ìœ ë¦¬) | ì—†ìŒ (ìƒëŒ€ì ìœ¼ë¡œ ê³µì •í•¨)         |
| **ëª¨ë¸ ì¢…ì†ì„±** | ìˆìŒ (íŠ¸ë¦¬ êµ¬ì¡°ì— ì˜ì¡´)      | ì—†ìŒ (ëª¨ë“  ëª¨ë¸ì— ì‚¬ìš© ê°€ëŠ¥)      |
| **í•´ì„ë ¥**    | ë‚®ìŒ (ë‚´ë¶€ êµ¬ì¡° ì¤‘ì‹¬)       | ë†’ìŒ (ì˜ˆì¸¡ì— ì‹¤ì œ ì˜í–¥ ê¸°ë°˜)      |


âœ… **Tree ê¸°ë°˜ Feature Importanceê°€ ë” ìœ ìš©í•œ ìˆœê°„**

| ìƒí™©                                                  | ì´ìœ                                  |
| --------------------------------------------------- | ---------------------------------- |
| ğŸ”¸ **ë¹ ë¥´ê²Œ í”¼ì²˜ ì¤‘ìš”ë„ í™•ì¸ì´ í•„ìš”í•  ë•Œ**                         | í›ˆë ¨ê³¼ ë™ì‹œì— ê³„ì‚°ë˜ë¯€ë¡œ ì†ë„ê°€ ë§¤ìš° ë¹ ë¦„            |
| ğŸ”¸ **ìˆ˜ë§ì€ í”¼ì²˜ê°€ ìˆì„ ë•Œ ê°„ë‹¨í•œ ìŠ¤í¬ë¦¬ë‹ìš©ìœ¼ë¡œ ì‚¬ìš©**                  | ê³„ì‚°ëŸ‰ ì ê³  ì´ˆê¸° ë¶„ì„ì— ì í•©                   |
| ğŸ”¸ **ëª¨ë¸ ë‚´ë¶€ì˜ ì‘ë™ ì›ë¦¬ë¥¼ ë¹ ë¥´ê²Œ íŒŒì•…í•˜ê³  ì‹¶ì„ ë•Œ**                  | íŠ¸ë¦¬ êµ¬ì¡° ê¸°ë°˜ìœ¼ë¡œ ì–´ë–¤ í”¼ì²˜ê°€ ì£¼ìš” ë¶„í•  ê¸°ì¤€ì´ì—ˆëŠ”ì§€ ë³´ì—¬ì¤Œ |
| ğŸ”¸ **ëª¨ë¸ ìì²´ê°€ íŠ¸ë¦¬ ê³„ì—´ì´ê³ , ê°„ë‹¨í•œ feature selectionì´ ëª©ì ì¼ ë•Œ** | íŠ¸ë¦¬ ëª¨ë¸ì— ìµœì í™”ëœ ë°©ì‹ìœ¼ë¡œ ë¹ ë¥´ê²Œ ì¤‘ìš”ë„ ë„ì¶œ ê°€ëŠ¥     |
| ğŸ”¸ **ê³„ì†í•´ì„œ ëª¨ë¸ì„ ë°˜ë³µí•´ì„œ íŠœë‹í•  ë•Œ**                          | ì†ë„ íš¨ìœ¨ì„± ë•Œë¬¸ì— ë°˜ë³µ ì‘ì—…ì— ë¶€ë‹´ì´ ì ìŒ           |

> âš ï¸ ë‹¨ì : **ë²”ì£¼ ìˆ˜ê°€ ë§ì€ ë³€ìˆ˜ë‚˜ ì—°ì†í˜• ë³€ìˆ˜ì— ê³¼ëŒ€í‰ê°€ë˜ëŠ” í¸í–¥ì´ ìˆìŒ**

---

âœ… **Permutation Importanceê°€ ë” ìœ ìš©í•œ ìˆœê°„**

| ìƒí™©                                               | ì´ìœ                               |
| ------------------------------------------------ | ------------------------------- |
| ğŸ”¹ **ëª¨ë¸ì´ ì‹¤ì œë¡œ í”¼ì²˜ì— ì–¼ë§ˆë‚˜ ì˜ì¡´í•˜ëŠ”ì§€ ì•Œê³  ì‹¶ì„ ë•Œ**             | ì˜ˆì¸¡ ì„±ëŠ¥ ì €í•˜ ê¸°ì¤€ìœ¼ë¡œ ê³„ì‚°ë˜ì–´ ì§ê´€ì           |
| ğŸ”¹ **ëª¨ë¸ í•´ì„ì´ ì¤‘ìš”í•œ ìƒí™© (ì„¤ëª… ê°€ëŠ¥í•œ AI, XAI)**            | ê° í”¼ì²˜ê°€ ê²°ê³¼ì— ë¯¸ì¹˜ëŠ” ì˜í–¥ì´ ë¶„ëª…í•˜ê²Œ ë“œëŸ¬ë‚¨      |
| ğŸ”¹ **ëª¨ë¸ì´ ë³µì¡í•˜ê±°ë‚˜, ë¹„íŠ¸ë¦¬ ê³„ì—´ì¼ ë•Œ (ex. Neural Network)** | ëª¨ë¸ê³¼ ë¬´ê´€í•˜ê²Œ ì ìš© ê°€ëŠ¥ (model-agnostic) |
| ğŸ”¹ **í”¼ì²˜ ê°„ ìƒê´€ê´€ê³„ë¥¼ í™•ì¸í•˜ê±°ë‚˜ íƒì§€í•  ë•Œ**                    | ìƒê´€ëœ í”¼ì²˜ì˜ ì¤‘ìš”ë„ ë³€í™”ë¡œ ì˜ì¡´ì„± íƒì§€ ê°€ëŠ¥       |
| ğŸ”¹ **ë¦¬ìŠ¤í¬ ê´€ë¦¬, ê·œì œ ìš”êµ¬ì‚¬í•­, ì˜ë£Œ ë“± í•´ì„ì´ ì¤‘ìš”í•œ ë¶„ì•¼**          | ì˜ˆì¸¡ ê¸°ë°˜ì´ë¯€ë¡œ ì‹¤ì œ í˜„ì—…ì—ì„œì˜ ì¤‘ìš”ë„ë¥¼ ë°˜ì˜í•¨      |

> âš ï¸ ë‹¨ì : **ê³„ì‚° ì†ë„ ëŠë¦¼, ìƒê´€ê´€ê³„ ìˆëŠ” ë³€ìˆ˜ëŠ” ê³¼ì†Œí‰ê°€ë  ìˆ˜ ìˆìŒ**

ğŸ’¡ ì •ë¦¬ ìš”ì•½: ì–´ë–¤ ìƒí™©ì— ì–´ë–¤ ë°©ë²•?

| ìƒí™©                          | ì¶”ì²œ ë°©ë²•                  |
| --------------------------- | ---------------------- |
| ëª¨ë¸ì´ íŠ¸ë¦¬ ê³„ì—´ì´ê³  ë¹ ë¥´ê²Œ ì¤‘ìš”ë„ ë³´ê³  ì‹¶ë‹¤   | íŠ¸ë¦¬ ê¸°ë°˜ Importance       |
| í”¼ì²˜ê°€ ëª¨ë¸ ì˜ˆì¸¡ì— ë¯¸ì¹˜ëŠ” ì‹¤ì œ ì˜í–¥ì´ ê¶ê¸ˆí•˜ë‹¤  | Permutation Importance |
| ëª¨ë¸ì´ ë¹„íŠ¸ë¦¬ ëª¨ë¸ì´ë‹¤ (ì˜ˆ: SVM, NN ë“±) | Permutation Importance |
| í•´ì„ë ¥ì´ ì¤‘ìš”í•˜ê±°ë‚˜ ì„¤ëª…ì´ í•„ìš”í•œ í”„ë¡œì íŠ¸     | Permutation Importance |
| ë§ì€ í”¼ì²˜ ì¤‘ ê°„ë‹¨íˆ ì“¸ëª¨ì—†ëŠ” í”¼ì²˜ ê±¸ëŸ¬ë‚´ê³  ì‹¶ë‹¤ | íŠ¸ë¦¬ ê¸°ë°˜ Importance       |



## ğŸ“Œ 5.2 êµì°¨ ê²€ì¦ê³¼ ê·¸ë¦¬ë“œ ì„œì¹˜

>###  ê²€ì¦ ì„¸íŠ¸
: í…ŒìŠ¤íŠ¸ ì„¸íŠ¸ë¥¼ ì‚¬ìš©í•˜ì§€ ì•Šê³  ê³¼ëŒ€/ê³¼ì†Œ ì í•©ì¸ì§€ íŒë‹¨í•˜ëŠ” ë°©ë²•, ë³´í†µ 6 : 2 : 2 ë¹„ìœ¨ë¡œ ì‚¬ìš©

**ë¨¼ì € 8:2ë¡œ í…ŒìŠ¤íŠ¸ ì„¸íŠ¸ ë¶„ë¦¬ ë’¤ í›ˆë ¨ ì„¸íŠ¸ì—ì„œ 20%ë¥¼ ë‹¤ì‹œ ë—´ì–´ ê²€ì¦ ì„¸íŠ¸ë¡œ ë§Œë“ ë‹¤**

```
from sklearn.model_selection import train_test_split
train_input, test_input, train_target, test_target = train_test_split(data, target, test_size=0.2, random_state=42)
```
* í•œë²ˆ ë” ë‚˜ëˆˆë‹¤
```
sub_input, val_input, sub_target, val_target = train_test_split(train_input, train_target, test_size=0.2, random_state=42)
print(sub_input.shape, val_input.shape, test_input.shape)
```
>### k-fold êµì°¨ ê²€ì¦ : ì•ˆì •ì ì¸ ê²€ì¦ ì„¸íŠ¸ ì ìˆ˜ë¥¼ ìœ„í•´ ì‚¬ìš©
* default : k = 5, cv ë§¤ê°œë³€ìˆ˜ë¡œ í´ë“œ ìˆ˜ ë³€ê²½ ê°€ëŠ¥
```
from sklearn.model_selection import cross_validate
scores = cross_validate(dt, train_input, train_target)
print(scores)
```

**ğŸ“Œ í›ˆë ¨ ì„¸íŠ¸ ì„ì„ë¼ë©´ ë¶„í• ê¸°(splitter) ì§€ì •**
>ë¶„ë¥˜ ëª¨ë¸ì˜ ê²½ìš° íƒ€ê¹ƒ í´ë˜ìŠ¤ê°€ ê³¨ê³ ë£¨ ì„ì´ê¸° ìœ„í•´ì„  StratifiedKFold ì‚¬ìš©

```
from sklearn.model_selection import StratifiedKFold
splitter = StratifiedKFold(n_splits=10, shuffle=True, random_state=42)
scores = cross_validate(dt, train_input, train_target, cv=splitter)
print(np.mean(scores['test_score']))  
```
**splitter = StratifiedKFold() ì‘ì„± í›„ cv = splitterë¡œ ë„£ê¸°**

>### í•˜ì´í¼ íŒŒë¼ë¯¸í„° íŠœë‹ by GridSearch CV

* GridSearchCV : í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹ + êµì°¨ ê²€ì¦

**ğŸ“ min_impurity_decrease ë§¤ê°œë³€ìˆ˜ì˜ ìµœì ê°’ ì°¾ì•„ë³´ê¸°**
```
from sklearn.model_selection import GridSearchCV
params = {'min_impurity_decrease' : [0.0001, 0.0002, 0.0003, 0.0004, 0.0005]}
gs = GridSearchCV(DecisionTreeClassifier(random_state=42), params, n_jobs=-1, cv=5)
gs.fit(train_input, train_target)
```
    * n_jobs : ë³‘ë ¬ ì‹¤í–‰ì— ì‚¬ìš©í•  ì¼ê¾¼(ì½”ì–´)ìˆ˜, -1ë¡œ ì„¤ì •ì‹œ ëª¨ë“  ì½”ì–´ ì‚¬ìš©

| í•­ëª©    | `gs.best_estimator_` | `gs.best_params_` |
| ----- | -------------------- | ----------------- |
| ì˜ë¯¸    | ìµœì  ëª¨ë¸ ê°ì²´             | ìµœì  í•˜ì´í¼íŒŒë¼ë¯¸í„° ì¡°í•©     |
| íƒ€ì…    | ëª¨ë¸ í´ë˜ìŠ¤ ì¸ìŠ¤í„´ìŠ¤          | ë”•ì…”ë„ˆë¦¬ (`dict`)     |
| ì‚¬ìš© ëª©ì  | ì˜ˆì¸¡ ë° í‰ê°€ì— ì‚¬ìš©          | ìµœì  íŒŒë¼ë¯¸í„° í™•ì¸ ë° ë¡œê¹…   |
| ì˜ˆì‹œ    | `model.predict()`    | `print(params)`   |


**ğŸ“ ì¢€ ë” ë³µì¡í•œ ì˜ˆì œ**

```
params = {'min_impurity_decrease' : np.arange(0.0001, 0.001, 0.0001), # 0.0001ë¶€í„° ì‹œì‘í•´ì„œ 0,001ê¹Œì§€ 0.0001 ëŠ˜ë¦¬ë©´ì„œ íƒìƒ‰
          'max_depth' : range(5,20,1),
          'min_samples_split': range(2,100,10)}
gs = GridSearchCV(DecisionTreeClassifier(random_state=42), params, n_jobs=-1, cv=5)
gs.fit(train_input, train_target) 
```
>### ëœë¤ì„œì¹˜ : ë§¤ê°œë³€ìˆ˜ê°’ ëª©ë¡(ë¦¬ìŠ¤íŠ¸) ê°€ ì•„ë‹Œ **í™•ë¥ ë¶„í¬ ê°ì²´**ë¥¼ ì „ë‹¬, ì‹œê°„ ì¤„ì–´ë“¬

* scipy.stats ë¼ì´ë¸ŒëŸ¬ë¦¬ì—ì„œ í™•ë¥  ë¶„í¬ ì„í¬íŠ¸ í›„ ì‚¬ìš©í•œë‹¤
```
params = {'min_impurity_decrease' : uniform(0.0001, 0.001), 
          'max_depth' :randint(20,50),
          'min_samples_split': randint(2,25),
          'min_samples_leaf': randint(1,25)}
```
* ìƒ˜í”Œë§íšŸìˆ˜ëŠ” n_iter ë§¤ê°œë³€ìˆ˜ì—ì„œ ì§€ì •í•¨

```
from sklearn.model_selection import RandomizedSearchCV
gs = RandomizedSearchCV(DecisionTreeClassifier(random_state=42), params, n_iter=100, n_jobs=-1, cv=5,random_state=42)
gs.fit(train_input, train_target)
```
```
dt = gs.best_estimator_
print(dt.score(test_input, test_target))
```

## ğŸ“Œ ì•™ìƒë¸”

>### ëœë¤í¬ë ˆìŠ¤íŠ¸

* ì •í˜• ë°ì´í„° : ë°ì´í„°ë² ì´ìŠ¤, ì—‘ì…€ë¡œ í‘œí˜„í•  ìˆ˜ ìˆëŠ” ë°ì´í„° -> ì•™ìƒë¸” í•™ìŠµì´ ê°€ì¥ ë›°ì–´ë‚œ ì„±ê³¼ë¥¼ ë³´ë‚¸ë‹¤

* ì•™ìƒë¸” ëª¨ë¸ì€ ëŒ€ë¶€ë¶„ ê²°ì •íŠ¸ë¦¬ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ë§Œë“¤ì–´ì ¸ ìˆìŒ

* ê°œë…
    1) ê²°ì • íŠ¸ë¦¬ë¥¼ ëœë¤í•˜ê²Œ ë§Œë“¤ì–´ ìˆ²ì„ ë§Œë“¬
    2) **ë¶€íŠ¸ìŠ¤íŠ¸ë© ìƒ˜í”Œ** í›ˆë ¨ ë°ì´í„°ì…‹ì„ ë³µì› ì¶”ì¶œí•˜ëŠ” ìƒ˜í”Œ, 1000ê°œì˜ ê°€ë°©ì—ì„œ 100ê°œì”© ìƒ˜í”Œì„ ë³µì›ì¶”ì¶œí•´ì„œ ì´ 1000ê°œ ë½‘ìŒ
    3) ê° ë…¸ë“œë¥¼ ë¶„í•  í•  ë•Œ ì „ì²´ í”¼ì³ ê°œìˆ˜ì˜ ì œê³±ê·¼ ë§Œí¼(4ê°œ í”¼ì³ë¼ë©´ 2ê°œ) ì„ íƒí•´ì„œ ë¶„í•  íŠ¹ì„± ì‚¬ìš©
        * ë‹¤ë§Œ RandomForestRegressorëŠ” ì „ì²´ íŠ¹ì„± ì‚¬ìš©
    4) ê° íŠ¸ë¦¬ì˜ í´ë˜ìŠ¤ë³„ í™•ë¥ ì„ í‰ê· í•˜ì—¬ ê°€ì¥ ë†’ì€ í™•ë¥ ì„ ê°€ì§„ í´ë˜ìŠ¤ë¥¼ ì˜ˆì¸¡ì¹˜ë¡œ ì‚¬ìš©
    5) ê³¼ì í•© ë°©ì§€, ì•ˆì •ì ì¸ ì„±ëŠ¥

* ì‹¤ìŠµ
    * n_jobs ëŠ” -1 ì„ ì‚¬ìš©í•˜ëŠ”ê²ƒì´ ì¢‹ë‹¤

```
from sklearn.ensemble import RandomForestClassifier
rf = RandomForestClassifier(n_jobs=-1, random_state=42)
scores = cross_validate(rf, train_input, train_target, return_train_score=True, n_jobs=-1)
print(np.mean(scores['train_score']), np.mean(scores['test_score']))
```

**ğŸ“OOB ìƒ˜í”Œ : ë¶€íŠ¸ìŠ¤íŠ¸ë© ìƒ˜í”Œì— í¬í•¨ë˜ì§€ ì•Šê³  ë‚¨ì€ ì• ë“¤ -> ê²€ì¦ì„¸íŠ¸ ì—­í•  ê°€ëŠ¥**
    -> oob_score =True ì„¤ì •

```
rf = RandomForestClassifier(oob_score=True, n_jobs=-1, random_state=42)
rf.fit(train_input, train_target)
print(rf.oob_score_)
```
>### ê·¸ë˜ë””ì–¸íŠ¸ ë¶€ìŠ¤íŒ…

* ê°œë…
    1) ê¹Šì´ê°€ ì–•ì€ ê²°ì •íŠ¸ë¦¬ë¥¼ ì‚¬ìš©í•˜ì—¬ ì´ì „ íŠ¸ë¦¬ ì˜¤ì°¨ ë³´ì™„
    2) ê²½ì‚¬ í•˜ê°•ë²•ì„ ì‚¬ìš©í•˜ì—¬ íŠ¸ë¦¬ë¥¼ ì•™ìƒë¸”ì— ì¶”ê°€
    3) ê³¼ì í•©ì— ë§¤ìš° ê°•í•¨

**gpt ì„¤ëª…**

```
# ğŸŒ² Gradient Boosting (ê·¸ë˜ë””ì–¸íŠ¸ ë¶€ìŠ¤íŒ…)

## ğŸ“Œ ê°œìš”

**ê·¸ë˜ë””ì–¸íŠ¸ ë¶€ìŠ¤íŒ…(Gradient Boosting)**ì€ ì—¬ëŸ¬ ê°œì˜ **ì•½í•œ í•™ìŠµê¸°(ì£¼ë¡œ ì‘ì€ ê²°ì • íŠ¸ë¦¬)**ë¥¼ ìˆœì°¨ì ìœ¼ë¡œ í›ˆë ¨ì‹œì¼œ, ì´ì „ ëª¨ë¸ì´ ë§Œë“  **ì˜¤ì°¨ë¥¼ ì¤„ì—¬ë‚˜ê°€ë©° ì„±ëŠ¥ì„ í–¥ìƒ**ì‹œí‚¤ëŠ” ì•™ìƒë¸” í•™ìŠµ ê¸°ë²•ì…ë‹ˆë‹¤.

> í•µì‹¬ ì•„ì´ë””ì–´:  
> **"ì´ì „ ëª¨ë¸ì´ í‹€ë¦° ë¶€ë¶„ì„ ë‹¤ìŒ ëª¨ë¸ì´ ì˜ˆì¸¡í•˜ë„ë¡ í›ˆë ¨í•˜ê³ , ê·¸ ê²°ê³¼ë¥¼ ì¡°ê¸ˆì”© ë”í•´ ì „ì²´ ì˜ˆì¸¡ì„ ë³´ì •í•œë‹¤."**
```

## ğŸ” ì‘ë™ ì›ë¦¬ (íšŒê·€ ì˜ˆì‹œ, ë¶„ë¥˜ ëª¨ë¸ì—ì„œëŠ” ì˜ˆì¸¡ê°’ê³¼ ì‹¤ì œê°’ ì‚¬ì´ì˜ ì†ì‹¤ í•¨ìˆ˜ì˜ ë¯¸ë¶„ê°’(= ê·¸ë˜ë””ì–¸íŠ¸)ì„ íƒ€ê¹ƒìœ¼ë¡œ ìƒˆ ëª¨ë¸ì„ í•™ìŠµ)

### Step 1: ì´ˆê¸° ì˜ˆì¸¡ê°’ ì„¤ì •
- ëª¨ë“  ìƒ˜í”Œì— ëŒ€í•´ ê°™ì€ ì´ˆê¸°ê°’ ì˜ˆì¸¡ (ì˜ˆ: í‰ê· ê°’)

```text
Å·â‚€ = ì „ì²´ yì˜ í‰ê· 
````

### Step 2: ì”ì°¨(residual) ê³„ì‚°

* ì‹¤ì œê°’ê³¼ ì˜ˆì¸¡ê°’ì˜ ì°¨ì´ = ëª¨ë¸ì´ í‹€ë¦° ì •ë„

```text
residual râ‚ = y - Å·â‚€
```

### Step 3: ì”ì°¨ë¥¼ ì˜ˆì¸¡í•˜ëŠ” ì•½í•œ ëª¨ë¸ í•™ìŠµ

* ìƒˆ íŠ¸ë¦¬ ëª¨ë¸ì„ í•™ìŠµì‹œì¼œ **râ‚ì„ ì˜ˆì¸¡**í•˜ê²Œ í•¨

### Step 4: ì˜ˆì¸¡ê°’ ì—…ë°ì´íŠ¸

* ì˜ˆì¸¡ê°’ì— ìƒˆë¡œìš´ ëª¨ë¸ì˜ ê²°ê³¼ë¥¼ **learning rateë§Œí¼ ê³±í•´ì„œ ë”í•¨**

```text
Å·â‚ = Å·â‚€ + Î± Ã— hâ‚(x)
```

* Î±ëŠ” **learning rate** (ì‘ì„ìˆ˜ë¡ í•™ìŠµ ì†ë„ëŠ” ëŠë¦¬ì§€ë§Œ ì•ˆì •ì )

### Step 5: ë°˜ë³µ

* ë§¤ë²ˆ ìƒˆ ì”ì°¨ë¥¼ ê³„ì‚°í•˜ê³ , ì”ì°¨ë¥¼ ì˜ˆì¸¡í•˜ëŠ” ìƒˆ ëª¨ë¸ì„ í•™ìŠµ â†’ ì˜ˆì¸¡ ë³´ì •
* ì „ì²´ ëª¨ë¸:

```text
Final prediction:
Å· = Å·â‚€ + Î±Â·hâ‚(x) + Î±Â·hâ‚‚(x) + Î±Â·hâ‚ƒ(x) + ... + Î±Â·hâ‚™(x)
```

---

## ğŸ“Š ì˜ˆì‹œ í…Œì´ë¸” (íšŒê·€)

| X | y (ì‹¤ì œê°’) | ì´ˆê¸° ì˜ˆì¸¡ Å·â‚€ | ì˜¤ì°¨ râ‚ | íŠ¸ë¦¬1 ì˜ˆì¸¡ | ì—…ë°ì´íŠ¸ëœ ì˜ˆì¸¡ Å·â‚          |
| - | ------- | -------- | ----- | ------ | -------------------- |
| A | 80      | 66.7     | +13.3 | +10    | 66.7 + 0.1Ã—10 = 67.7 |
| B | 50      | 66.7     | -16.7 | -15    | 66.7 - 1.5 = 65.2    |
| C | 70      | 66.7     | +3.3  | +5     | 66.7 + 0.5 = 67.2    |

---

## âœ… ì¥ì 

* ğŸ”¥ **ê³ ì„±ëŠ¥**: ë§ì€ ì‹¤ì œ ë¬¸ì œì—ì„œ ë›°ì–´ë‚œ ì˜ˆì¸¡ë ¥
* ğŸ” **ìœ ì—°í•¨**: íšŒê·€, ì´ì§„ ë¶„ë¥˜, ë‹¤ì¤‘ ë¶„ë¥˜ ë“± ë‹¤ì–‘í•œ ì†ì‹¤ í•¨ìˆ˜ ì‚¬ìš© ê°€ëŠ¥
* ğŸ“‰ **ìë™ íŠ¹ì„± ì„ íƒ**: ì¤‘ìš”í•˜ì§€ ì•Šì€ ë³€ìˆ˜ëŠ” ë¬´ì‹œë˜ëŠ” ê²½í–¥
* ğŸ§  **ê³¼ì í•© ì œì–´ ê°€ëŠ¥**: í•™ìŠµë¥ , íŠ¸ë¦¬ ìˆ˜, íŠ¸ë¦¬ ê¹Šì´ ì¡°ì ˆë¡œ ì¡°ì • ê°€ëŠ¥

---

## âš ï¸ ë‹¨ì 

* ğŸ¢ **í•™ìŠµ ì†ë„ ëŠë¦¼**: íŠ¸ë¦¬ë¥¼ ìˆœì°¨ì ìœ¼ë¡œ í•™ìŠµí•˜ë¯€ë¡œ ë³‘ë ¬í™” ì–´ë ¤ì›€
* ğŸ›  **í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹ í•„ìš”**: ì„±ëŠ¥ ìµœì í™”ë¥¼ ìœ„í•´ ì„¸ë°€í•œ ì¡°ì • í•„ìš”
* ğŸ­ **ë³µì¡ì„± ë†’ìŒ**: ëª¨ë¸ í•´ì„ì´ ì–´ë ¤ì›€ (í•˜ì§€ë§Œ feature importanceë¡œ ì¼ë¶€ í•´ê²° ê°€ëŠ¥)
* ğŸ¤• **ê³¼ì í•© ìœ„í—˜**: íŠ¸ë¦¬ ìˆ˜ê°€ ë§ê±°ë‚˜ ê³¼í•˜ê²Œ ê¹Šìœ¼ë©´ ê³¼ì í•©ë  ìˆ˜ ìˆìŒ

---

## ğŸš€ ëŒ€í‘œ êµ¬í˜„ ë¼ì´ë¸ŒëŸ¬ë¦¬

| ë¼ì´ë¸ŒëŸ¬ë¦¬            | íŠ¹ì§•                          |
| ---------------- | --------------------------- |
| **XGBoost**      | ë¹ ë¥´ê³  ì„±ëŠ¥ ìš°ìˆ˜, ì •êµí•œ íŠœë‹ ê°€ëŠ¥        |
| **LightGBM**     | ëŒ€ìš©ëŸ‰ ë°ì´í„°ì— ìµœì í™”, ë¹ ë¦„            |
| **CatBoost**     | ë²”ì£¼í˜• ë°ì´í„° ì²˜ë¦¬ì— ê°•í•¨              |
| **scikit-learn** | ê¸°ë³¸ì ì¸ GradientBoosting êµ¬í˜„ ì œê³µ |

---

## ğŸ“Œ ì°¸ê³  í‚¤ì›Œë“œ

* **Residual**: ì‹¤ì œê°’ê³¼ ì˜ˆì¸¡ê°’ì˜ ì°¨ì´ â†’ ë‹¤ìŒ ëª¨ë¸ì´ ì˜ˆì¸¡í•´ì•¼ í•  íƒ€ê¹ƒ
* **Learning Rate (Î±)**: ê° íŠ¸ë¦¬ì˜ ê¸°ì—¬ë„ë¥¼ ì¡°ì ˆ
* **Boosting**: ì•½í•œ ëª¨ë¸ì„ ëª¨ì•„ ê°•í•œ ëª¨ë¸ì„ ë§Œë“œëŠ” ë°©ì‹

---

## ğŸ“ ê´€ë ¨ ê°œë… ë¹„êµ

| ê°œë…           | ì„¤ëª…                                |
| ------------ | --------------------------------- |
| **Bagging**  | ë³‘ë ¬ì ìœ¼ë¡œ ì—¬ëŸ¬ ëª¨ë¸ í•™ìŠµ (ex: ëœë¤ í¬ë ˆìŠ¤íŠ¸)      |
| **Boosting** | ìˆœì°¨ì ìœ¼ë¡œ ëª¨ë¸ í•™ìŠµ, ì´ì „ ëª¨ë¸ì˜ ì˜¤ì°¨ë¥¼ ë‹¤ìŒ ëª¨ë¸ì´ ë³´ì • |
| **Stacking** | ì—¬ëŸ¬ ëª¨ë¸ì˜ ì˜ˆì¸¡ì„ ë©”íƒ€ ëª¨ë¸ì´ ì¡°í•©              |

---

* ì‹¤ìŠµ

```
from sklearn.ensemble import GradientBoostingClassifier
gb = GradientBoostingClassifier(random_state=42, n_estimators=500, learning_rate=0.2)
scores = cross_validate(gb, train_input, train_target, return_train_score=True, n_jobs=-1)
print(np.mean(scores['train_score']), np.mean(scores['test_score']))
```
-> treeë¥¼ 500ê°œë‚˜ ì‚¬ìš©í–ˆì§€ë§Œ ê³¼ì í•©ì´ ì˜ ë˜ì§€ ì•Šê³  ì„±ëŠ¥ì´ ì¢‹ìŒ

#### Gradient Boosting  ëª¨ë¸ë“¤

> ğŸ“ XGB
```
from xgboost import XGBClassifier
xgb =XGBClassifier(tree_method = 'hist', random_state=42)
scroes = cross_validate(xgb, train_input, train_target, return_train_score=True, n_jobs=-1)
print(np.mean(scores['train_score']), np.mean(scores['test_score']))  
```
> ğŸ“ LightGBM
```
from lightgbm import LGBMClassifier
lgb = LGBMClassifier(random_state=42)
scores = cross_validate(lgb, train_input, train_target, return_train_score=True, n_jobs=-1)
print(np.mean(scores['train_score']), np.mean(scores['test_score']))
```
fin.
