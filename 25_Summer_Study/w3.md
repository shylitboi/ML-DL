# 6. 비지도 학습

## 📌 군집 알고리즘

* 비지도학습 : 타겟이 없을때 분류하는 머신러닝 알고리즘

>### 과일 데이터
* (300,100,100) 크기 = 300개의 샘플, 100*100크기의 이미지, 100씩 사과, 파인애플, 바나나
* 맷플롯립의 imshow() 함수를 통해 넘파이 배열로 저장된 이미지를 그릴 수 있음
    * 흑백이므로(컬러는 4차원 텐서) cmap 매개변수를 gray

```python
plt.imshow(fruits[0], cmap='gray') # 밝은 부분은 255(숫자)에 가까움 -> 실제와는 괴리가 있음 -> gray_r 
plt.show()
```
![](../25_Summer_Study/images/w3/fruit.png)

```python
plt.imshow(fruits[0], cmap='gray_r')
plt.show()
```
![](../25_Summer_Study/images/w3/fruit2.png)

>### 픽셀값 분석하기
* 넘파이 배열을 나눌 때 100*100 이미지를 10000인 1차원 배열로 만들기 for 계산
1) 과일이 각 100개씩 있으므로 100개씩 슬라이싱
2) 슬라이싱을 통해 2, 3차원을 10000으로 합침

```python
apple = fruits[:100].reshape(-1, 100*100)
pineapple = fruits[100:200].reshape(-1, 100*100)
banana = fruits[200:].reshape(-1, 100*100)
```
* 각 과일의 픽셀 평균값 계산하기
    * axis = 0 : 행을 따라 계산(하나의 열에대해 계산)
    * axis =1 : 열 따라 계싼(하나의 행에 대해 계산)
    * 우리가 필요한건 샘플의 평균값, 우리는 샘플을 가로로 나열(1* 10000)했으므로 행기준 계산이 필요

```python
# axis =1 로 하나의 행에 대해 픽셀값 평균 계산
print(apple.mean(axis=1))
```

```python
# 히스토그램
plt.hist(np.mean(apple, axis=1),alpha = 0.8)
plt.hist(np.mean(pineapple, axis=1), alpha = 0.8)
plt.hist(np.mean(banana, axis=1), alpha = 0.8)
```

![](../25_Summer_Study/images/w3/fruit3.png)
✅ 바나나는 낮은 필셀값, 사과와 파인애플은 픽셀값만으로 구분하기 쉽지 않음
* why?: 파인애플과 사과는 사진이 크고 동그랗기 때문. 바나나는 사진이 작고 길쭉한 차이
* 해결 방법 : 샘플의 평균값이 아니
라 픽셀별 평균값 = 전체 샘플에 대해 각 픽셀(0~9999)의 평균값 => axis = 0 으로 지정하면 됨

```python
# 전체 샘플에 대해 각 픽셀별 평균값 비교
fix, axes = plt.subplots(1,3,figsize=(20,5))
axes[0].bar(range(10000), np.mean(apple, axis=0))
axes[1].bar(range(10000), np.mean(pineapple, axis=0))
axes[2].bar(range(10000), np.mean(banana, axis=0))
```
![](../25_Summer_Study/images/w3/fruits4.png)

 ✅ 과일마다 높은 구간이 다름, 바나나는 확실히 중앙의 픽셀값이 높음

 * 픽셀의 평균값읓 100*100 크기로 바꾸어 이미지로 출력
 ```python
 apple_mean = np.mean(apple, axis=0).reshape(100,100)
pineapple_mean = np.mean(pineapple, axis=0).reshape(100,100)
banana_mean = np.mean(banana, axis=0).reshape(100,100)
fig, axes = plt.subplots(1,3,figsize=(20,5))
axes[0].imshow(apple_mean, cmap='gray_r')
axes[1].imshow(pineapple_mean, cmap='gray_r')
axes[2].imshow(banana_mean, cmap='gray_r')
plt.show()
```
![](../25_Summer_Study/images/w3/fruits5.png)

>### 평균값과 가까운 사진 고르기
* 사과 사진의 평균값인 apple_mean(100*100 array)과 가장 가까운 사진 고르기 by abs() 함수
```python
abs_diff = np.abs(fruits - apple_mean)
abs_mean = np.mean(abs_diff, axis=(1,2))
print(abs_mean.shape) # 300개 사진에 대해 사과 평균과 차이이므로 (300,)인 1차원 배열
```

* 이 값이 가장 작은 순서대로 100개 고르기  = 가장 비슷한 사진 100개 고르기
    * np.argsort() 작은것에서 큰 순서대로 나열한 인덱스 반환
```python
apple_index =  np.argsort(abs_mean)[:100]
fig, axes = plt.subplots(10,10,figsize=(10,10)) # subplot 100개 생성
for i in range(10): # 행
  for j in range(10): # 열
    axes[i,j].imshow(fruits[apple_index[i*10+j]],cmap = 'gray_r')
    axes[i,j].axis('off') # axis('off') 깔끔하게 그리기 위해 좌표축 그리지 않음
plt.show()
```
![](../25_Summer_Study/images/w3/fruits6.png)

>### 실제 비지도 학습
: 실제로는 타깃값을 모르기 때문에 데이터에 있는 패턴을 찾아야함
* 군집 : 비슷한 샘플끼리 그룹으로 모으는 작업
* 다음절에서는 타깃이 없는 사진 사용할것

## 📌 k-means clustering

>### intro
* 앞의 예시에선 사과, 파인, 바나나 사진임을 이미 알고 있었음. 하지만 진짜 비지도 학습에서는 어떤 과일이 있는지 모름 => k-means 알고리즘 사용

* k-means : 평균값이 클러스터 중심에 위치

>### K-Means 알고리즘 소개
1) 무작위로 k개 클러스터 중심 정함
2) 각 샘플에서 가장 가까운 클러스터 중심을 찾아 해당 클러스터에 배정
3) 클러스터에 속한 샘플의 평균값으로 클러스터 중심 변경
4) 클러스터 중심에 변화가 없을 때까지 반복

>### 구현

```python
fruits_2d = fruits.reshape(-1, 100*100)
from sklearn.cluster import KMeans
km = KMeans(n_clusters=3, random_state=42)
km.fit(fruits_2d)
print(km.labels_)
```
* 3개의 클러스터(n_clusters 메서드 사용)
* 군집의 결과는 labels_속성에 저장됨
* 각 레이블마다 몇개가 있는지 확인 : ```print(np.unique(km.labels_, return_counts=True))```
* 각 클러스터가 어떤 그림을 나타냈는지 출력 
    : figsize는 ratio에 비례하여 커짐
    ```python
    def draw_fruits(arr, ratio=1):
        n = len(arr) 
        rows = int(np.ceil(n/10)) # 행 개수
        cols = n if rows<2 else 10
        fig, axs = plt.subplots(rows, cols, figsize=(cols*ratio, rows*ratio), squeeze=False)
        for i in range(rows):
            for j in range(cols):
                if i*10 + j <n:
                    axs[i,j].imshow(arr[i*10+j], cmap='gray_r')
                axs[i,j].axis('off')
        plt.show()
    ```
    * 레이블 0(클러스터 1) 사진 출력 by ```km.labels_ == 0```
    
    ```python
    draw_fruits(fruits[km.labels_==0])
    ```
![]()
-> 완벽하게 구별하진 못하지만 그래도 스스로 비슷한 샘플을 잘 모음
>### 클러스터 중심
* 최종 클러스터의 중심은 ```cluster_centers_``` 속성에 저장됨
```python
draw_fruits(km.cluster_centers_.reshape(-1, 100, 100), ratio=3)
```
* 가장 가까운 클러스터로 예측 by predict()
```python
print(km.predict(fruits_2d[100:101]))
```

>### 최적의 k 찾기
✅ **엘보우 method**
: 클러스터 중심과 샘플의 거리의 제곱합 = 이니셔라고 정의
* 일반적으로 클러스터가 늘어나면 이니셔가 줄어듬
* 이때 클러스터 개수를 증가시키면서 이니셔를 그래프로 그리면 감소하는 속도가 꺽이는 지점 존재 => 이 지점부터 클러ㅗ스터에 잘 밀집된 정도가 개선되지 않음 = 

* ```inertia_``` 사용하여 계산
    * k를 2~6까지 변화시키면서 kmeans 5번 훈련
```python
inertia = []
for k in range(2,7):
  km = KMeans(n_clusters=k, random_state=42)
  km.fit(fruits_2d)
  inertia.append(km.inertia_)
plt.plot(range(2,7), inertia)
plt.xlabel('k')
plt.ylabel('inertia')
plt.show()
```
-> k = 3 결정!